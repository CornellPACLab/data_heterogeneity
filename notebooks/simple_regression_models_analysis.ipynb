{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Regression Models Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Timestamp\n",
    "import numpy\n",
    "from scipy.stats import shapiro\n",
    "from statsmodels.genmod.generalized_estimating_equations import GEE\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM\n",
    "import statsmodels.api as sm\n",
    "from importlib import reload\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pingouin as pg\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.lines import Line2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import regression_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosscheck = pd.read_csv('../data/crosscheck_daily_data_cleaned_w_sameday.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentlife = pd.read_csv('../data/studentlife_daily_data_cleaned_w_sameday_08282021.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMA cols\n",
    "ema_cols_crosscheck = [i for i in crosscheck.columns if 'ema' in i]\n",
    "ema_cols_studentlife = [i for i in studentlife.columns if 'ema' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavior cols\n",
    "behavior_cols_crosscheck = [\n",
    "    i for i in crosscheck.columns if i not in ['study_id', 'eureka_id', 'date'] + ema_cols_crosscheck\n",
    "]\n",
    "\n",
    "behavior_cols_studentlife = [\n",
    "    i for i in studentlife.columns if i not in ['study_id', 'eureka_id', 'day'] + ema_cols_studentlife\n",
    "]\n",
    "\n",
    "behavior_cols = list(set(behavior_cols_crosscheck) & set(behavior_cols_studentlife))\n",
    "behavior_cols.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = behavior_cols[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosscheck_temp = crosscheck.copy()\n",
    "crosscheck_temp[behavior_cols] = crosscheck_temp[behavior_cols].fillna(0) # Not using the columns with NAs. All \n",
    "                                                                          # ambient audio/light\n",
    "\n",
    "features = [f for f in features if len(crosscheck_temp[f].unique()) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform StudentLife EMA to look like CrossCheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentlife_temp = studentlife[['study_id', 'day'] + \n",
    "    behavior_cols + ['ema_Stress_level', 'ema_Sleep_rate']\n",
    "].reset_index(drop=True).copy() # TEMP FILL\n",
    "\n",
    "# Fill NA\n",
    "non_sleep_loc_cols = [i for i in behavior_cols if ('loc' not in i) and ('sleep' not in i)]\n",
    "studentlife_temp[non_sleep_loc_cols] = studentlife_temp[non_sleep_loc_cols].fillna(0)\n",
    "\n",
    "# Fill sleep with average value for that individual\n",
    "for s in studentlife_temp.study_id.unique():\n",
    "    temp = studentlife_temp.loc[studentlife_temp.study_id == s, :]\n",
    "    duration_mean = temp['sleep_duration'].mean()\n",
    "    start_mean = temp['sleep_start'].mean()\n",
    "    end_mean = temp['sleep_end'].mean()\n",
    "    ind = (studentlife_temp.study_id == s) & pd.isnull(studentlife_temp['sleep_duration'])\n",
    "    studentlife_temp.loc[ind, 'sleep_duration'] = duration_mean\n",
    "    studentlife_temp.loc[ind, 'sleep_start'] = start_mean\n",
    "    studentlife_temp.loc[ind, 'sleep_end'] = end_mean\n",
    "\n",
    "# Drop days without location (14 total) and days still w/o sleep (all IDs with no sleep info)\n",
    "studentlife_temp = studentlife_temp.dropna(subset=behavior_cols).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to map all of them from 0-3\n",
    "\n",
    "# Stress [1]A little stressed, [2]Definitely stressed, [3]Stressed out, [4]Feeling good, [5]Feeling great, \n",
    "studentlife_temp['ema_STRESSED'] = studentlife_temp['ema_Stress_level'].map({\n",
    "    5:0, 4:0, 1:1, 2:2, 3:3\n",
    "})\n",
    "# Map from 0 - 3\n",
    "minimum = studentlife_temp['ema_STRESSED'].min()\n",
    "maximum = studentlife_temp['ema_STRESSED'].max()\n",
    "studentlife_temp['ema_STRESSED'] =  3 * (studentlife_temp['ema_STRESSED'] - minimum) / (maximum - minimum)\n",
    "\n",
    "# Sleeping [1]Very good, [2]Fairly good, [3]Fairly bad, [4]Very bad, \n",
    "# Map from 0 - 3\n",
    "studentlife_temp['ema_SLEEPING'] = 4 - studentlife_temp['ema_Sleep_rate'].copy().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['ema_SLEEPING', 'ema_STRESSED']\n",
    "studentlife_temp['data'] = 'sl'\n",
    "crosscheck_temp['data']= 'cc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosscheck_temp['day'] = pd.to_datetime(crosscheck_temp['date']).dt.tz_localize('US/Eastern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make study IDs dataset specific\n",
    "crosscheck_temp['study_id'] = 'cc' + crosscheck_temp['study_id'].astype(str)\n",
    "studentlife_temp['study_id'] = 'sl' + studentlife_temp['study_id'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base models CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSO entire population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loso_res_df = pd.read_csv('../res/loso_res_df_all_v10_10182021.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "y_mean = []\n",
    "mae_list = []\n",
    "days_list = []\n",
    "study_id = []\n",
    "target = []\n",
    "model_type = []\n",
    "params = []\n",
    "data = []\n",
    "smote_list = []\n",
    "neighbors_list = []\n",
    "\n",
    "for ind in loso_res_df.index:\n",
    "    s = loso_res_df.loc[ind, 'fold']\n",
    "    t = loso_res_df.loc[ind, 'target']\n",
    "    days_list += pd.to_datetime(eval(loso_res_df.loc[ind, 'day']))\n",
    "    y_true += eval(loso_res_df.loc[ind, 'y_true'])\n",
    "    y_pred += eval(loso_res_df.loc[ind, 'y_pred'])\n",
    "    y_mean += eval(loso_res_df.loc[ind, 'y_mean'])\n",
    "    study_id += [loso_res_df.loc[ind, 'fold']] \\\n",
    "        * len(eval(loso_res_df.loc[ind, 'y_true']))\n",
    "    mae_list += [loso_res_df.loc[ind, 'mae']] \\\n",
    "        * len(eval(loso_res_df.loc[ind, 'y_true']))\n",
    "    smote_list += [loso_res_df.loc[ind, 'smote']] \\\n",
    "        * len(eval(loso_res_df.loc[ind, 'y_true']))\n",
    "    target += [loso_res_df.loc[ind, 'target']] \\\n",
    "        * len(eval(loso_res_df.loc[ind, 'y_true']))\n",
    "    model_type += [loso_res_df.loc[ind, 'model_type']] \\\n",
    "        * len(eval(loso_res_df.loc[ind, 'y_true']))\n",
    "    params += [loso_res_df.loc[ind, 'params']] \\\n",
    "        * len(eval(loso_res_df.loc[ind, 'y_true']))\n",
    "    data += [loso_res_df.loc[ind, 'data']] \\\n",
    "        * len(eval(loso_res_df.loc[ind, 'y_true']))\n",
    "    neighbors_list += [loso_res_df.loc[ind, 'neighbors']] \\\n",
    "        * len(eval(loso_res_df.loc[ind, 'y_true']))\n",
    "    \n",
    "df_overall_err = pd.DataFrame({\n",
    "    'study_id': study_id,\n",
    "    'day': days_list,\n",
    "    'mae': mae_list,\n",
    "    'smote': smote_list,\n",
    "    'neighbors': neighbors_list,\n",
    "    'y_true': y_true,\n",
    "    'y_pred': y_pred,\n",
    "    'y_mean': y_mean,\n",
    "    'target': target,\n",
    "    'model_type': model_type,\n",
    "    'params': params,\n",
    "    'train_data': data\n",
    "})\n",
    "\n",
    "df_overall_err['day'] = pd.to_datetime(df_overall_err['day'], utc=True)\n",
    "\n",
    "df_overall_err.loc[df_overall_err.y_pred >= 3, 'y_pred'] = 3\n",
    "df_overall_err.loc[df_overall_err.y_pred <= 0, 'y_pred'] = 0\n",
    "\n",
    "df_overall_err['dataset'] = [i[0:2] for i in df_overall_err.study_id]\n",
    "\n",
    "df_overall_err['ae'] = np.abs(df_overall_err['y_true'] - df_overall_err['y_pred'])\n",
    "df_overall_err['baseline_ae'] = np.abs(df_overall_err['y_true'] - df_overall_err['y_mean'])\n",
    "\n",
    "df_overall_err.loc[pd.isnull(df_overall_err.neighbors), 'neighbors'] = 'None'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between training data results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run tests to validate if better than means, each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_pivot = pd.pivot_table(\n",
    "    df_overall_err, index=['target', 'dataset', 'model_type', 'params', 'smote', 'neighbors', 'study_id', 'day'], \n",
    "    columns=['train_data'], values=['ae', 'baseline_ae']\n",
    ").reset_index().fillna(0)\n",
    "df_overall_pivot.columns = [\n",
    "    i[0] if i[1] == '' else i[0] + '_' + i[1] for i in df_overall_pivot.columns]\n",
    "df_overall_pivot['ae_single'] = df_overall_pivot['ae_cc'] + df_overall_pivot['ae_sl']\n",
    "df_overall_pivot['baseline_ae'] = df_overall_pivot['baseline_ae_cc'] + df_overall_pivot['baseline_ae_sl']\n",
    "df_overall_pivot['ae_diff'] = df_overall_pivot['ae_single'] - df_overall_pivot['ae_both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_pivot_drop_dup = df_overall_pivot[\n",
    "    ['target', 'model_type', 'params', 'smote', 'neighbors', 'dataset']\n",
    "].drop_duplicates().reset_index(drop=True)\n",
    "df_overall_pivot_drop_dup['baseline_single_res'] = None\n",
    "df_overall_pivot_drop_dup['baseline_single_pval'] = None\n",
    "df_overall_pivot_drop_dup['baseline_single_shap_pval'] = None\n",
    "df_overall_pivot_drop_dup['baseline_both_res'] = None\n",
    "df_overall_pivot_drop_dup['baseline_both_pval'] = None\n",
    "df_overall_pivot_drop_dup['baseline_both_shap_pval'] = None\n",
    "df_overall_pivot_drop_dup['res'] = None\n",
    "df_overall_pivot_drop_dup['pval'] = None\n",
    "df_overall_pivot_drop_dup['shap_pval'] = None\n",
    "df_overall_pivot_drop_dup['reverse_res'] = None\n",
    "df_overall_pivot_drop_dup['reverse_pval'] = None\n",
    "df_overall_pivot_drop_dup['reverse_shap_pval'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_pivot_drop_dup.groupby(['target', 'dataset'])['model_type'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate better than baseline\n",
    "total = df_overall_pivot_drop_dup.shape[0]\n",
    "\n",
    "for ind in df_overall_pivot_drop_dup.index.values:\n",
    "    temp = pd.merge(\n",
    "        left=df_overall_pivot_drop_dup.loc[[ind], :],\n",
    "        right=df_overall_pivot,\n",
    "        on=['target', 'model_type', 'params', 'smote', 'neighbors', 'dataset']\n",
    "    )\n",
    "    \n",
    "    # Single\n",
    "    p_shap = shapiro(temp['baseline_ae'] - temp['ae_single'])[1]\n",
    "    res, p_test = util.rosner_test(temp, 'baseline_ae', 'ae_single', 'study_id')\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'baseline_single_res'] = res\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'baseline_single_pval'] = p_test\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'baseline_single_shap_pval'] = p_shap\n",
    "    \n",
    "    # Both\n",
    "    p_shap = shapiro(temp['baseline_ae'] - temp['ae_both'])[1]\n",
    "    res, p_test = util.rosner_test(temp, 'baseline_ae', 'ae_both', 'study_id')\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'baseline_both_res'] = res\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'baseline_both_pval'] = p_test\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'baseline_both_shap_pval'] = p_shap\n",
    "    \n",
    "    \n",
    "    # Between both and single\n",
    "    p_shap = shapiro(temp['ae_single'] - temp['ae_both'])[1]\n",
    "    res, p_test = util.rosner_test(temp, 'ae_single', 'ae_both', 'study_id')\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'res'] = res\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'pval'] = p_test\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'shap_pval'] = p_shap\n",
    "    \n",
    "    # Reverse both and single\n",
    "    p_shap = shapiro(temp['ae_both'] - temp['ae_single'])[1]\n",
    "    res, p_test = util.rosner_test(temp, 'ae_both', 'ae_single', 'study_id')\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'reverse_res'] = res\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'reverse_pval'] = p_test\n",
    "    df_overall_pivot_drop_dup.loc[ind, 'reverse_shap_pval'] = p_shap\n",
    "    \n",
    "    print(total - ind, \n",
    "          df_overall_pivot_drop_dup.loc[ind, 'pval'], df_overall_pivot_drop_dup.loc[ind, 'reverse_pval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_overall_pivot_drop_dup.to_csv('../res/df_overall_pivot_drop_dup_sig_v10_10192021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_pivot_drop_dup = pd.read_csv('../res/df_overall_pivot_drop_dup_sig_v10_10192021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_dict = {\n",
    "    'baseline_single_pval': 'baseline_ae_single_sig',\n",
    "    'baseline_both_pval': 'baseline_ae_both_sig',\n",
    "    'reverse_pval': 'single_sig',\n",
    "    'pval': 'both_sig',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sig_dict.keys():\n",
    "    df_overall_pivot_drop_dup[sig_dict[c]] = df_overall_pivot_drop_dup[c] < .05\n",
    "\n",
    "\n",
    "df_overall_pivot_drop_dup['both_and'] = \\\n",
    "    df_overall_pivot_drop_dup.baseline_ae_both_sig & df_overall_pivot_drop_dup.both_sig\n",
    "df_overall_pivot_drop_dup['single_and'] = \\\n",
    "    df_overall_pivot_drop_dup.baseline_ae_single_sig & df_overall_pivot_drop_dup.single_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BH Correction\n",
    "q = 0.25\n",
    "\n",
    "for c in sig_dict.keys():\n",
    "    df_overall_pivot_drop_dup = df_overall_pivot_drop_dup.sort_values(by=c).reset_index(drop=True)\n",
    "    df_overall_pivot_drop_dup['rank'] = range(1, df_overall_pivot_drop_dup.shape[0] + 1)\n",
    "    df_overall_pivot_drop_dup['bh'] = (df_overall_pivot_drop_dup['rank'] / df_overall_pivot_drop_dup.shape[0]) * q\n",
    "    max_rank = df_overall_pivot_drop_dup.loc[\n",
    "        df_overall_pivot_drop_dup[c] < df_overall_pivot_drop_dup['bh'], 'rank'].max()\n",
    "    df_overall_pivot_drop_dup[sig_dict[c] + '_bh'] = 0\n",
    "    df_overall_pivot_drop_dup.loc[df_overall_pivot_drop_dup['rank'] <= max_rank, sig_dict[c] + '_bh'] = 1\n",
    "    \n",
    "df_overall_pivot_drop_dup['both_and_bh'] = \\\n",
    "    df_overall_pivot_drop_dup.baseline_ae_both_sig_bh & df_overall_pivot_drop_dup.both_sig_bh\n",
    "df_overall_pivot_drop_dup['single_and_bh'] = \\\n",
    "    df_overall_pivot_drop_dup.baseline_ae_single_sig_bh & df_overall_pivot_drop_dup.single_sig_bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_pivot_drop_dup.shape[0] / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_pivot_drop_dup.groupby(['target', 'dataset'])[[\n",
    "    'both_sig', 'single_sig',  'baseline_ae_both_sig', 'baseline_ae_single_sig', 'both_and', 'single_and',\n",
    "]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_pivot_drop_dup.groupby(['target', 'dataset'])[[\n",
    "    'both_sig', 'single_sig',  'baseline_ae_both_sig', 'baseline_ae_single_sig', 'both_and', 'single_and',\n",
    "]].mean().round(2) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_pivot_drop_dup.groupby(['target', 'dataset'])[[\n",
    "    'both_sig_bh', 'single_sig_bh',  'baseline_ae_both_sig_bh', \n",
    "    'baseline_ae_single_sig_bh', 'both_and_bh', 'single_and_bh',\n",
    "]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_pivot_drop_dup.groupby(['target', 'dataset'])[[\n",
    "    'both_sig_bh', 'single_sig_bh',  'baseline_ae_both_sig_bh', \n",
    "    'baseline_ae_single_sig_bh', 'both_and_bh', 'single_and_bh',\n",
    "]].mean().round(2) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_grouped_by_study_id = df_overall_err.groupby(\n",
    "    ['target', 'model_type', 'params', 'smote', 'neighbors', 'dataset', 'train_data', 'study_id'], as_index=False\n",
    ")['ae', 'baseline_ae'].mean()\n",
    "\n",
    "df_overall_err_grouped_by_model = df_overall_err_grouped_by_study_id.groupby(\n",
    "    ['target', 'model_type', 'params', 'smote', 'neighbors', 'dataset', 'train_data'], as_index=False\n",
    ")['ae', 'baseline_ae'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_grouped_by_study_id_pivot = pd.pivot_table(\n",
    "    df_overall_err_grouped_by_study_id,\n",
    "    index=['target', 'model_type', 'smote', 'neighbors', 'params', 'dataset', 'study_id'],\n",
    "    columns=['train_data'],\n",
    "    values=['ae', 'baseline_ae']\n",
    ").reset_index().fillna(0)\n",
    "df_overall_err_grouped_by_study_id_pivot.columns = [\n",
    "    i[0] if i[1] == '' else i[0] + '_' + i[1] for i in df_overall_err_grouped_by_study_id_pivot.columns.values\n",
    "]\n",
    "df_overall_err_grouped_by_study_id_pivot['ae_single'] = \\\n",
    "    df_overall_err_grouped_by_study_id_pivot['ae_cc'] + df_overall_err_grouped_by_study_id_pivot['ae_sl']\n",
    "\n",
    "df_overall_err_grouped_by_study_id_pivot['baseline_ae_single'] = \\\n",
    "    df_overall_err_grouped_by_study_id_pivot['baseline_ae_cc'] + \\\n",
    "    df_overall_err_grouped_by_study_id_pivot['baseline_ae_sl']\n",
    "\n",
    "df_overall_err_grouped_by_study_id_pivot.drop(\n",
    "    ['ae_cc', 'ae_sl', 'baseline_ae_cc', 'baseline_ae_sl'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df_overall_err_grouped_by_study_id_pivot['mae_diff'] = \\\n",
    "    df_overall_err_grouped_by_study_id_pivot['ae_single'] - df_overall_err_grouped_by_study_id_pivot['ae_both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_grouped_by_model_pivot = pd.pivot_table(\n",
    "    df_overall_err_grouped_by_model,\n",
    "    index=['target', 'model_type', 'smote', 'neighbors', 'params', 'dataset'],\n",
    "    columns=['train_data'],\n",
    "    values=['ae', 'baseline_ae']\n",
    ").reset_index().fillna(0)\n",
    "df_overall_err_grouped_by_model_pivot.columns = [\n",
    "    i[0] if i[1] == '' else i[0] + '_' + i[1] for i in df_overall_err_grouped_by_model_pivot.columns.values\n",
    "]\n",
    "df_overall_err_grouped_by_model_pivot['ae_single'] = \\\n",
    "    df_overall_err_grouped_by_model_pivot['ae_cc'] + df_overall_err_grouped_by_model_pivot['ae_sl']\n",
    "\n",
    "df_overall_err_grouped_by_model_pivot['baseline_ae_single'] = \\\n",
    "    df_overall_err_grouped_by_model_pivot['baseline_ae_cc'] + \\\n",
    "    df_overall_err_grouped_by_model_pivot['baseline_ae_sl']\n",
    "\n",
    "df_overall_err_grouped_by_model_pivot.drop(\n",
    "    ['ae_cc', 'ae_sl', 'baseline_ae_cc', 'baseline_ae_sl'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df_overall_err_grouped_by_model_pivot['mae_diff'] = \\\n",
    "    df_overall_err_grouped_by_model_pivot['ae_single'] - df_overall_err_grouped_by_model_pivot['ae_both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_grouped_by_model_pivot_test = df_overall_err_grouped_by_model_pivot.groupby([\n",
    "    'target', 'dataset'\n",
    "], as_index=False).apply(\n",
    "    util.grouped_paired_test, col1='ae_single', col2='ae_both').sort_values(\n",
    "    by=['target', 'dataset']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_grouped_by_model_pivot_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In cases where differ by .01, graph\n",
    "plt.figure(figsize=(20, 20))\n",
    "curr = 1\n",
    "\n",
    "param_cols = ['target', 'model_type', 'smote', 'neighbors', 'params', 'dataset']\n",
    "\n",
    "percentiles = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "palette = sns.color_palette(\"colorblind\", n_colors=4)\n",
    "palette2 = sns.color_palette(\"cubehelix\", n_colors=2)\n",
    "\n",
    "target_map = {'ema_SLEEPING': 'EMA: Sleep', 'ema_STRESSED': 'EMA: Stress'}\n",
    "dataset_map = {'cc': 'CrossCheck', 'sl': 'StudentLife'}\n",
    "\n",
    "for t in df_overall_err_grouped_by_model_pivot.target.unique():\n",
    "    for d in df_overall_err_grouped_by_model_pivot.dataset.unique():\n",
    "        temp = df_overall_err_grouped_by_model_pivot.loc[\n",
    "            (df_overall_err_grouped_by_model_pivot.target == t) &\n",
    "            (df_overall_err_grouped_by_model_pivot.dataset == d), :].copy().sort_values(by='dataset')\n",
    "        \n",
    "        # Get num_ids\n",
    "        num_ids = df_overall_err_grouped_by_study_id.loc[\n",
    "            (df_overall_err_grouped_by_study_id.target == t) &\n",
    "            (df_overall_err_grouped_by_study_id.dataset == d), 'study_id'\n",
    "        ].nunique()\n",
    "        \n",
    "        # Get quantiles\n",
    "        mae_diff = []\n",
    "        single_mae = []\n",
    "        both_mae = []\n",
    "        baseline_single_mae = []\n",
    "        baseline_both_mae = []\n",
    "        percentiles_list = []\n",
    "        for q in percentiles:\n",
    "            params = temp.loc[temp.mae_diff == temp.mae_diff.quantile(q=q, interpolation='nearest'), param_cols\n",
    "                             ].iloc[[0], :]\n",
    "            temp2 = pd.merge(left=params, right=df_overall_err_grouped_by_study_id_pivot)\n",
    "            \n",
    "            single_mae += list(temp2['ae_single'])\n",
    "            both_mae += list(temp2['ae_both'])\n",
    "            baseline_single_mae += list(temp2['baseline_ae_single'])\n",
    "            baseline_both_mae += list(temp2['baseline_ae_both'])\n",
    "            mae_diff += list(temp2['mae_diff'])\n",
    "            percentiles_list += [q] * num_ids\n",
    "\n",
    "        ax = plt.subplot(4, 1, curr)\n",
    "\n",
    "        ax = sns.lineplot(x=percentiles_list, y=mae_diff, color=palette[2], lw=5, ax=ax)\n",
    "        plt.title('Dataset: ' + dataset_map[d] + ', ' + target_map[t], size=16, pad=20)\n",
    "        \n",
    "        if curr < 4:\n",
    "            plt.xlabel('')\n",
    "        else:\n",
    "            plt.xlabel('Percentile', labelpad=20, size=16)\n",
    "\n",
    "\n",
    "        plt.ylabel('$\\Delta$MAE', labelpad=20, size=16)\n",
    "\n",
    "        plt.xticks(size=14)\n",
    "        plt.yticks(size=14)\n",
    "        ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        \n",
    "        mae_dff_df = pd.DataFrame({\n",
    "            'mae_diff': np.abs(mae_diff),\n",
    "            'percentiles': percentiles_list\n",
    "        })\n",
    "        \n",
    "        mae_dff_df_grouped = mae_dff_df.groupby(['percentiles'])['mae_diff'].mean()\n",
    "\n",
    "        plt.axhline(y=0, color='k', ls='dashed')\n",
    "        plt.axvline(x=mae_dff_df_grouped.idxmin(), color='k', ls='dashed')\n",
    "\n",
    "        # Add marks\n",
    "        temp_test = df_overall_err_grouped_by_model_pivot_test.loc[\n",
    "            (df_overall_err_grouped_by_model_pivot_test.target == t) &\n",
    "            (df_overall_err_grouped_by_model_pivot_test.dataset == d), :\n",
    "        ].copy()\n",
    "        max_val = ax.get_ylim()[1]\n",
    "        temp_test.index = temp_test.dataset\n",
    "        if temp_test.loc[d, 'test'] == 'W':\n",
    "            stat = 'W=' + str(int(temp_test.loc[d, 'W-val'])) + '\\n'\n",
    "            effect = '\\n$RBC=' + str(temp_test.loc[d, 'effect'].round(2)) + '$'\n",
    "        else:\n",
    "            effect = '\\n$Cohen-d=' + str(temp_test.loc[d, 'effect'].round(2)) + '$'\n",
    "        if temp_test.loc[d, 'pval'] < .001:\n",
    "            plt.text(s=stat + 'p<.001' + effect, x=0.675, y=max_val * 0.7, size=16)\n",
    "        elif temp_test.loc[d, 'pval'] < .05:\n",
    "            plt.text(\n",
    "                s=stat + 'p=' + str(temp_test.loc[d, 'pval'].round(3))[1:] + effect, \n",
    "                x=0.675, y=max_val * 0.7, size=16)\n",
    "        elif temp_test.loc[d, 'pval'] < .1:\n",
    "            plt.text(s=stat + 'p<.1' + effect, x=0.675, y=max_val * 0.7, size=16)\n",
    "        \n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        quantile_df = pd.DataFrame({\n",
    "            'percentile': percentiles_list *2,\n",
    "            'MAE': both_mae + single_mae ,\n",
    "            'Training Data': ['Combined'] * len(both_mae) + [dataset_map[d]] * len(single_mae),\n",
    "            'style': ['-'] * len(both_mae) + ['--'] * len(single_mae),\n",
    "        })\n",
    "        ax2 = sns.lineplot(x=quantile_df['percentile'], y=quantile_df['MAE'], hue=quantile_df['Training Data'],\n",
    "                      palette=palette[:2], ax=ax2, lw=2, style=quantile_df['style'], alpha=1)\n",
    "        plt.axhline(y=np.mean(baseline_single_mae), color=palette[-1], ls=':', alpha=1, lw=2)\n",
    "        \n",
    "        plt.xlabel('')\n",
    "        \n",
    "        \n",
    "        if curr == 3:\n",
    "            custom_lines = [\n",
    "                Line2D([0], [0], color=palette[2], lw=5, ls='-'),\n",
    "                Line2D([0], [0], color='k', lw=3, ls='dashed', alpha=1),\n",
    "                Line2D([0], [0], color=palette[0], lw=3, ls='-', alpha=1),\n",
    "                Line2D([0], [0], color=palette[1], lw=3, ls='--', alpha=1),\n",
    "                Line2D([0], [0], color=palette[-1], lw=3, ls=':', alpha=1),\n",
    "            ]\n",
    "            ax2.legend(custom_lines, \n",
    "                       ['$\\Delta$MAE', 'Percentile $\\Delta$MAE = 0',\n",
    "                        'Combined MAE', 'Single-Study MAE', 'Baseline Mean MAE'], \n",
    "                       prop={'size': 16}, frameon=False)\n",
    "        else:\n",
    "            ax2.get_legend().remove()\n",
    "        \n",
    "        plt.xticks(size=14)\n",
    "        plt.yticks(size=14)\n",
    "        ax2.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "        plt.ylabel('MAE', labelpad=20, size=16, rotation=-90)\n",
    "        curr += 1\n",
    "    \n",
    "        \n",
    "sns.despine(right=False)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../../CrossDataset_Paper/Figures/Fig5.tiff', \n",
    "            dpi=300, format='tiff', facecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proxy-A Dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proxy-A Dist Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "study_id_list = []\n",
    "train_data_list = []\n",
    "smote_list = []\n",
    "neighbors_list = []\n",
    "proxy_a_dist_list = []\n",
    "emd_list = []\n",
    "\n",
    "for target in targets:\n",
    "    for smote in [False, True]:\n",
    "        for neighbors in [None, 5, 10, 50, 100, 500]:\n",
    "            for train_data in ['cc', 'sl', 'both']:\n",
    "                if train_data == 'sl':\n",
    "                    data = studentlife_temp.copy()\n",
    "                elif train_data == 'cc':\n",
    "                    data = crosscheck_temp.copy()\n",
    "                else:\n",
    "                    data = pd.concat(\n",
    "                        [crosscheck_temp, studentlife_temp], axis=0).reset_index(drop=True) \n",
    "                for t, v in zip(*regression_cv.get_loso_cv_data(\n",
    "                    data=data, features=features, target=target\n",
    "                )):\n",
    "\n",
    "                    train_norm, val_norm = regression_cv.scale_data(\n",
    "                        train=t, val=v, features=features, \n",
    "                        target=target, neighbors=neighbors, smote=smote\n",
    "                    )\n",
    "                    # Get proxy a distance\n",
    "                    proxy_a_dist = util.proxy_a_distance(\n",
    "                        d1=train_norm, d2=val_norm, features=features)\n",
    "                    \n",
    "                    # Get EMD\n",
    "                    emd = util.calculate_emd(d1=train_norm[features], d2=val_norm[features], center=False)\n",
    "\n",
    "                    target_list.append(target)\n",
    "                    study_id_list.append(v.study_id.unique()[0])\n",
    "                    train_data_list.append(train_data)\n",
    "                    smote_list.append(smote)\n",
    "                    neighbors_list.append(neighbors)\n",
    "                    proxy_a_dist_list.append(proxy_a_dist)\n",
    "                    emd_list.append(emd)\n",
    "\n",
    "                    print(\n",
    "                        target, train_data, v.study_id.unique()[0], smote, \n",
    "                        neighbors, proxy_a_dist, emd\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = pd.DataFrame({\n",
    "    'target': target_list,\n",
    "    'study_id': study_id_list,\n",
    "    'train_data': train_data_list,\n",
    "    'smote': smote_list,\n",
    "    'neighbors': neighbors_list,\n",
    "    'proxy_a_dist': proxy_a_dist_list,\n",
    "    'emd': emd_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_df.to_csv('../res/dist_v10_10192021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df = pd.read_csv('../res/dist_v10_10192021.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_df.loc[pd.isnull(dist_df.neighbors), 'neighbors'] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_dist = df_overall_err.groupby(\n",
    "    ['target', 'smote', 'neighbors', 'study_id', 'dataset', 'train_data'], as_index=False)[['ae']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_dist = pd.merge(\n",
    "    left=df_overall_err_dist,\n",
    "    right=dist_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per Participant Proxy A Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_dist_pivot = pd.pivot_table(\n",
    "    df_overall_err_dist,\n",
    "    index=['target', 'smote', 'neighbors', 'dataset', 'study_id'],\n",
    "    columns=['train_data'],\n",
    "    values=['ae', 'proxy_a_dist']\n",
    ").reset_index().fillna(0)\n",
    "df_overall_err_dist_pivot.columns = [\n",
    "    i[0] if i[1] == '' else i[0] + '_' + i[1] for i in df_overall_err_dist_pivot.columns\n",
    "]\n",
    "df_overall_err_dist_pivot['ae_single'] = df_overall_err_dist_pivot['ae_cc'] + df_overall_err_dist_pivot['ae_sl']\n",
    "df_overall_err_dist_pivot['mae_diff'] = df_overall_err_dist_pivot['ae_single'] - df_overall_err_dist_pivot['ae_both']\n",
    "df_overall_err_dist_pivot['proxy_a_dist_single'] = \\\n",
    "    df_overall_err_dist_pivot['proxy_a_dist_cc'] + df_overall_err_dist_pivot['proxy_a_dist_sl']\n",
    "df_overall_err_dist_pivot['proxy_a_dist_diff'] = \\\n",
    "    df_overall_err_dist_pivot['proxy_a_dist_single'] - df_overall_err_dist_pivot['proxy_a_dist_both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'neighbors_10.0' not in df_overall_err_dist_pivot.columns.values:\n",
    "    df_overall_err_dist_pivot = pd.concat([\n",
    "        df_overall_err_dist_pivot,\n",
    "        pd.get_dummies(df_overall_err_dist_pivot[['target', 'neighbors', 'dataset']], drop_first=True)\n",
    "    ], axis=1)\n",
    "df_overall_err_dist_pivot['bias'] = 1\n",
    "\n",
    "cols = ['proxy_a_dist_diff', 'target_ema_STRESSED',\n",
    "       'neighbors_10.0',\n",
    "       'neighbors_50.0', 'neighbors_100.0', 'neighbors_500.0',\n",
    "       'neighbors_None', 'bias']\n",
    "\n",
    "gee = GEE(\n",
    "    endog=df_overall_err_dist_pivot['mae_diff'],\n",
    "    exog=df_overall_err_dist_pivot[cols],\n",
    "    groups=df_overall_err_dist_pivot['study_id'],\n",
    "    cov_struct=sm.cov_struct.Exchangeable()\n",
    ")\n",
    "gee_res = gee.fit()\n",
    "gee_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'neighbors_10.0' not in df_overall_err_dist_pivot.columns.values:\n",
    "    df_overall_err_dist_pivot = pd.concat([\n",
    "        df_overall_err_dist_pivot,\n",
    "        pd.get_dummies(df_overall_err_dist_pivot[['target', 'neighbors', 'dataset']], drop_first=True)\n",
    "    ], axis=1)\n",
    "df_overall_err_dist_pivot['bias'] = 1\n",
    "\n",
    "cols = ['proxy_a_dist_diff', 'target_ema_STRESSED',\n",
    "       'neighbors_10.0',\n",
    "       'neighbors_50.0', 'neighbors_100.0', 'neighbors_500.0',\n",
    "       'neighbors_None', 'bias']\n",
    "\n",
    "gee = GEE(\n",
    "    endog=df_overall_err_dist_pivot['mae_diff'],\n",
    "    exog=df_overall_err_dist_pivot[cols],\n",
    "    groups=df_overall_err_dist_pivot['study_id'],\n",
    "    cov_struct=sm.cov_struct.Exchangeable()\n",
    ")\n",
    "gee_res = gee.fit()\n",
    "gee_res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixedlm = MixedLM(\n",
    "    endog=df_overall_err_dist_pivot['mae_diff'],\n",
    "    exog=df_overall_err_dist_pivot[cols],\n",
    "    groups=df_overall_err_dist_pivot['study_id'],\n",
    ")\n",
    "mixedlm_res = mixedlm.fit()\n",
    "mixedlm_res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_grouped = df_overall_err.groupby(\n",
    "    ['target', 'model_type', 'params', 'smote', 'neighbors', 'dataset', 'train_data', 'study_id'], as_index=False\n",
    ")['ae'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 24))\n",
    "curr = 1\n",
    "\n",
    "target_map = {'ema_SLEEPING': 'EMA: Sleep', 'ema_STRESSED': 'EMA: Stress'}\n",
    "dataset_map = {'cc': 'CrossCheck', 'sl': 'StudentLife'}\n",
    "\n",
    "palette = sns.color_palette(\"colorblind\", n_colors=4)\n",
    "\n",
    "for t in df_overall_err_dist.target.unique():\n",
    "    for d in df_overall_err_dist.dataset.unique():\n",
    "        temp = df_overall_err_dist.loc[(df_overall_err_dist.target == t) & \n",
    "                                          (df_overall_err_dist.dataset == d), :].copy()\n",
    "        temp.loc[temp['neighbors'] == 'None', 'neighbors'] = 'All'\n",
    "        temp['train_data'] = temp['train_data'].map({'sl': 'StudentLife', 'cc': 'CrossCheck', 'both': 'Combined'})\n",
    "        temp = pd.merge(\n",
    "            left=pd.DataFrame({'neighbors': [5, 10, 50, 100, 500, 'All']}),\n",
    "            right=temp\n",
    "        )\n",
    "        plt.subplot(8, 2, curr)\n",
    "        \n",
    "        if curr == 1:\n",
    "            plt.text(x=-1.7, y=2, s='A', size=20, weight='bold')\n",
    "                \n",
    "        ax = sns.pointplot(\n",
    "            x=temp['neighbors'], y=temp['proxy_a_dist'], \n",
    "            hue=temp['train_data'], linestyles=[\"-\", \"--\"], markers=['o', 'v'], palette=palette[:2],\n",
    "            dodge=True, estimate='mean'\n",
    "        )\n",
    "        \n",
    "        plt.legend(title='Training Dataset', prop={'size': 12}, frameon=False)\n",
    "        \n",
    "        if curr in [1, 2]:\n",
    "            plt.title(dataset_map[d], size=16, pad=20)\n",
    "            \n",
    "        if curr in [1, 3]:\n",
    "            plt.ylabel(target_map[t] + '\\n\\nProxy-A Distance', labelpad=20, size=16)\n",
    "        else:\n",
    "            plt.ylabel('')\n",
    "            \n",
    "\n",
    "        plt.xlabel('')\n",
    "            \n",
    "            \n",
    "        ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        plt.xticks(size=14)\n",
    "        plt.yticks(size=14)\n",
    "        \n",
    "        curr += 1\n",
    "\n",
    "for t in df_overall_err_grouped.target.unique():\n",
    "    for d in df_overall_err_grouped.dataset.unique():\n",
    "        temp = df_overall_err_grouped.loc[(df_overall_err_grouped.target == t) & \n",
    "                                          (df_overall_err_grouped.dataset == d), :].copy()\n",
    "        temp.loc[temp['neighbors'] == 'None', 'neighbors'] = 'All'\n",
    "        temp['train_data'] = temp['train_data'].map({'sl': 'StudentLife', 'cc': 'CrossCheck', 'both': 'Combined'})\n",
    "        temp = pd.merge(\n",
    "            left=pd.DataFrame({'neighbors': [5, 10, 50, 100, 500, 'All']}),\n",
    "            right=temp\n",
    "        )\n",
    "        plt.subplot(8, 2, curr)\n",
    "        \n",
    "        if curr == 5:\n",
    "            plt.text(x=-1.7, y=0.98, s='B', size=20, weight='bold')\n",
    "                \n",
    "        ax = sns.pointplot(\n",
    "            x=temp['neighbors'], y=temp['ae'], \n",
    "            hue=temp['train_data'], linestyles=[\"-\", \"--\"], markers=['o', 'v'], palette=palette[:2],\n",
    "            dodge=True, estimate='mean'\n",
    "        )\n",
    "        \n",
    "        plt.legend(title='Training Dataset', prop={'size': 12}, frameon=False)\n",
    "            \n",
    "        if curr in [5, 7]:\n",
    "            plt.ylabel(target_map[t] + '\\n\\nMAE', labelpad=20, size=16)\n",
    "        else:\n",
    "            plt.ylabel('')\n",
    "            \n",
    "        if curr in [7, 8]:\n",
    "            plt.xlabel('Number of Neighbors', labelpad=20, size=16)\n",
    "        else:\n",
    "            plt.xlabel('')\n",
    "            \n",
    "            \n",
    "        ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        plt.xticks(size=14)\n",
    "        plt.yticks(size=14)\n",
    "        \n",
    "        curr += 1\n",
    "\n",
    "        \n",
    "sns.despine(right=False)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('../../CrossDataset_Paper/Figures/Fig6.tiff', dpi=300, format='tiff', facecolor='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smote effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err['y_pred_round'] = df_overall_err['y_pred'].round()\n",
    "\n",
    "df_overall_err_var = df_overall_err.groupby(\n",
    "    ['target', 'model_type', 'params', 'smote', 'neighbors', 'dataset', 'train_data', 'y_pred_round'\n",
    "    ], as_index=False\n",
    ")['day'].count().rename(columns={'day': 'value_counts'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add class values\n",
    "df_overall_err['y_pred_class'] = 0\n",
    "df_overall_err['y_true_class'] = 0\n",
    "\n",
    "df_overall_err.loc[\n",
    "    (df_overall_err.target == 'ema_SLEEPING') & (df_overall_err.y_true <= 1), 'y_true_class'\n",
    "] = 1\n",
    "df_overall_err.loc[\n",
    "    (df_overall_err.target == 'ema_STRESSED') & (df_overall_err.y_true >= 2), 'y_true_class'\n",
    "] = 1\n",
    "\n",
    "df_overall_err.loc[\n",
    "    (df_overall_err.target == 'ema_SLEEPING') & (df_overall_err.y_pred_round <= 1), 'y_pred_class'\n",
    "] = 1\n",
    "df_overall_err.loc[\n",
    "    (df_overall_err.target == 'ema_STRESSED') & (df_overall_err.y_pred_round >= 2), 'y_pred_class'\n",
    "] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_sensitivity = df_overall_err.groupby(\n",
    "    ['target', 'model_type', 'params', 'smote', 'neighbors', 'dataset', 'train_data'], as_index=False\n",
    ").apply(util.grouped_sensitivity, y_true='y_true_class', y_pred='y_pred_class').rename(columns={None: 'value'})\n",
    "\n",
    "df_overall_err_ppv = df_overall_err.groupby(\n",
    "    ['target', 'model_type', 'params', 'smote', 'neighbors', 'dataset', 'train_data'], as_index=False\n",
    ").apply(util.grouped_ppv, y_true='y_true_class', y_pred='y_pred_class').rename(columns={None: 'value'})\n",
    "\n",
    "df_overall_err_specificity = df_overall_err.groupby(\n",
    "    ['target', 'model_type', 'params', 'smote', 'neighbors', 'dataset', 'train_data'], as_index=False\n",
    ").apply(util.grouped_specificity, y_true='y_true_class', y_pred='y_pred_class').rename(columns={None: 'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_sensitivity['metric'] = 'Sensitivity'\n",
    "df_overall_err_ppv['metric'] = 'PPV'\n",
    "df_overall_err_specificity['metric'] = 'Specificity'\n",
    "\n",
    "df_overall_err_mae = df_overall_err_grouped.groupby([\n",
    "    'target', 'model_type', 'params', 'smote', 'neighbors', 'dataset', 'train_data'\n",
    "], as_index=False)['ae'].mean()\n",
    "df_overall_err_mae['metric'] = 'MAE'\n",
    "df_overall_err_mae.rename(columns={'ae': 'value'}, inplace=True)\n",
    "\n",
    "df_overall_err_metrics = pd.concat([\n",
    "    df_overall_err_sensitivity, df_overall_err_ppv, df_overall_err_specificity, \n",
    "    df_overall_err_mae\n",
    "]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify differences\n",
    "df_overall_err_metrics_pivot = pd.pivot_table(\n",
    "    data=df_overall_err_metrics,\n",
    "    index=['target', 'model_type', 'params', 'neighbors', 'dataset', 'train_data', 'metric'],\n",
    "    columns=['smote'],\n",
    "    values=['value']\n",
    ").reset_index()\n",
    "df_overall_err_metrics_pivot.columns = [\n",
    "    i[0] if i[1] == '' else str(i[0]) + '_' + str(i[1]) for i in df_overall_err_metrics_pivot.columns.values\n",
    "]\n",
    "\n",
    "df_overall_err_metrics_pivot_test = df_overall_err_metrics_pivot.groupby([\n",
    "    'target', 'metric', 'dataset'\n",
    "], as_index=False).apply(util.grouped_paired_test, col1='value_True', col2='value_False'\n",
    "                        ).sort_values(\n",
    "    by=['target', 'dataset', 'metric']\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overall_err_metrics_pivot_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "curr = 1\n",
    "\n",
    "palette = sns.color_palette(\"colorblind\", n_colors=4)\n",
    "\n",
    "target_map = {'ema_SLEEPING': 'EMA: Sleep', 'ema_STRESSED': 'EMA: Stress'}\n",
    "dataset_map = {'cc': 'CrossCheck', 'sl': 'StudentLife'}\n",
    "\n",
    "for t in df_overall_err_metrics.target.unique():\n",
    "    for d in df_overall_err_metrics.dataset.unique():\n",
    "        temp = df_overall_err_metrics.loc[(df_overall_err_metrics.target == t) & \n",
    "                                          (df_overall_err_metrics.dataset == d), :\n",
    "                                         ].copy().sort_values(by=['metric']).reset_index(drop=True)\n",
    "        temp['train_data'] = temp['train_data'].map({'sl': 'StudentLife', 'cc': 'CrossCheck', 'both': 'Combined'})\n",
    "        \n",
    "        temp = pd.merge(\n",
    "            left=pd.DataFrame({'metric': ['MAE', 'Specificity', 'Sensitivity', 'PPV']}),\n",
    "            right=temp\n",
    "        )\n",
    "        \n",
    "        plt.subplot(2, 2, curr)\n",
    "                \n",
    "        ax = sns.barplot(\n",
    "            x=temp['metric'], y=temp['value'], \n",
    "            hue=temp['smote'], palette=palette[:2],\n",
    "            capsize=0.1\n",
    "        )\n",
    "        \n",
    "        \n",
    "        plt.legend(title='SMOTE Used?', prop={'size': 12}, frameon=False, loc=(0.8, 0.8))\n",
    "        \n",
    "        if curr in [1, 2]:\n",
    "            plt.title(dataset_map[d], size=16, pad=20)\n",
    "            plt.xlabel('')\n",
    "        else:\n",
    "            plt.xlabel('Metric', labelpad=20, size=16)\n",
    "            \n",
    "        if curr in [1, 3]:\n",
    "            plt.ylabel(target_map[t] + '\\n\\nValue', labelpad=20, size=16)\n",
    "        else:\n",
    "            plt.ylabel('')\n",
    "            \n",
    "        plt.xticks(size=14)\n",
    "        plt.yticks(size=14)\n",
    "        ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "        \n",
    "        # Add marks\n",
    "        temp_pivot = df_overall_err_metrics_pivot_test.loc[\n",
    "            (df_overall_err_metrics_pivot_test.target == t) & (df_overall_err_metrics_pivot_test.dataset == d), :\n",
    "        ].copy()\n",
    "        max_val = ax.get_ylim()[1]\n",
    "        temp_pivot.index = temp_pivot.metric\n",
    "        \n",
    "        \n",
    "        if (temp_pivot.loc['MAE', 'pval'] > .95) or (temp_pivot.loc['MAE', 'pval'] < .05):\n",
    "            plt.text(s='*', x=0, y=temp.loc[temp.metric == 'MAE', 'value'].quantile(0.9), size=20)\n",
    "        elif (temp_pivot.loc['MAE', 'pval'] > .9) or (temp_pivot.loc['MAE', 'pval'] < .1):\n",
    "            plt.text(s='â€ ', x=0, y=temp.loc[temp.metric == 'MAE', 'value'].quantile(0.9), size=20)\n",
    "            \n",
    "        if (temp_pivot.loc['Specificity', 'pval'] < .05) or (temp_pivot.loc['Specificity', 'pval'] > .95):\n",
    "            plt.text(s='*', x=1, y=temp.loc[temp.metric == 'Specificity', 'value'].quantile(0.9), size=20)\n",
    "        elif (temp_pivot.loc['Specificity', 'pval'] < .1) or (temp_pivot.loc['Specificity', 'pval'] > .9):\n",
    "            plt.text(s='â€ ', x=1, y=temp.loc[temp.metric == 'Specificity', 'value'].quantile(0.9), size=20)\n",
    "            \n",
    "        if (temp_pivot.loc['Sensitivity', 'pval'] < .05) or (temp_pivot.loc['Sensitivity', 'pval'] > .95):\n",
    "            plt.text(s='*', x=2, y=temp.loc[temp.metric == 'Sensitivity', 'value'].quantile(0.9), size=20)\n",
    "        elif (temp_pivot.loc['Sensitivity', 'pval'] < .1) or (temp_pivot.loc['Sensitivity', 'pval'] > .9):\n",
    "            plt.text(s='â€ ', x=2, y=temp.loc[temp.metric == 'Sensitivity', 'value'].quantile(0.9), size=20)\n",
    "        \n",
    "        if (temp_pivot.loc['PPV', 'pval'] < .05) or (temp_pivot.loc['PPV', 'pval'] > .95):\n",
    "            plt.text(s='*', x=3, y=temp.loc[temp.metric == 'PPV', 'value'].quantile(0.9), size=20)\n",
    "        elif (temp_pivot.loc['PPV', 'pval'] < .1) or (temp_pivot.loc['PPV', 'pval'] > .9):\n",
    "            plt.text(s='â€ ', x=3, y=temp.loc[temp.metric == 'PPV', 'value'].quantile(0.9), size=20)\n",
    "              \n",
    "        \n",
    "        curr += 1\n",
    "        \n",
    "sns.despine()\n",
    "plt.tight_layout(h_pad=5)\n",
    "\n",
    "plt.savefig('../../CrossDataset_Paper/Figures/Fig7.tiff', dpi=300, format='tiff', facecolor='white')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
